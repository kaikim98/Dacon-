{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC7vaQMVyxRd",
        "outputId": "0b93e66d-8291-4720-d513-e24f0e886a22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-transformers\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/176.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (1.22.4)\n",
            "Collecting boto3 (from pytorch-transformers)\n",
            "  Downloading boto3-1.26.162-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.9/135.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (4.65.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2022.10.31)\n",
            "Collecting sentencepiece (from pytorch-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses (from pytorch-transformers)\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->pytorch-transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->pytorch-transformers) (16.0.6)\n",
            "Collecting botocore<1.30.0,>=1.29.162 (from boto3->pytorch-transformers)\n",
            "  Downloading botocore-1.29.162-py3-none-any.whl (11.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch-transformers)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->pytorch-transformers)\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.162->boto3->pytorch-transformers) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->pytorch-transformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->pytorch-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=ae2def8de479f018b98da4c72080a727301fee39e7c857dbaeb9793b8e74c30c\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, jmespath, botocore, s3transfer, boto3, pytorch-transformers\n",
            "Successfully installed boto3-1.26.162 botocore-1.29.162 jmespath-1.0.1 pytorch-transformers-1.2.0 s3transfer-0.6.1 sacremoses-0.0.53 sentencepiece-0.1.99\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pytorch-transformers\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "seed=42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "UnkRhiBXzBxe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/gdrive')\n",
        "file_path = '/content/gdrive/MyDrive/dacon law/'\n",
        "train_data = pd.read_csv(file_path+'train.csv')\n",
        "test_data = pd.read_csv(file_path+'test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzmEAyrgzEaV",
        "outputId": "6db692f0-c324-4e47-b4c1-d9e9c09abcc7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.drop(\"ID\", axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "HzQL0gNwzFw7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "val_rate=0.2\n",
        "train_indices, val_indices = train_test_split(range(len(train_data)), test_size=val_rate)"
      ],
      "metadata": {
        "id": "aAgLAE_azG-8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Subset\n",
        "train_dataset = train_data.iloc[train_indices]\n",
        "validation_dataset = train_data.iloc[val_indices]\n",
        "\n",
        "train_dataset.reset_index(drop=False, inplace=True)\n",
        "validation_dataset.reset_index(drop=False, inplace=True)"
      ],
      "metadata": {
        "id": "CgvPPFEOzJAM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "device = torch.device(\"cuda\")\n",
        "from transformers import BertTokenizer, BertModel, AdamW\n",
        "import transformers\n",
        "transformers.logging.set_verbosity_error()\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Fa69DFoYuk5A"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CaseDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, first_parties, second_parties, case_contents, labels):\n",
        "        self.first_parties = first_parties\n",
        "        self.second_parties = second_parties\n",
        "        self.case_contents = case_contents\n",
        "        self.labels = labels\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.first_parties)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            self.first_parties[idx],\n",
        "            self.case_contents[idx],\n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        label = torch.tensor(self.labels[idx])\n",
        "        return {\n",
        "            'input_ids': torch.tensor(inputs['input_ids'].squeeze(), device=device),\n",
        "            'attention_mask': torch.tensor(inputs['attention_mask'].squeeze(), device=device),\n",
        "            'labels': torch.tensor(label, device=device)\n",
        "        }"
      ],
      "metadata": {
        "id": "BMCm_2lWuocH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training parameters\n",
        "batch_size = 4\n",
        "num_epochs = 20\n",
        "learning_rate = 2e-5"
      ],
      "metadata": {
        "id": "_9TYGFRuuttw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset tuning\n",
        "train_first_parties=train_dataset['first_party']\n",
        "train_second_parties=train_dataset['second_party']\n",
        "train_case_contents=train_dataset['facts']\n",
        "train_labels=train_dataset['first_party_winner']\n",
        "\n",
        "train_dataset_ = CaseDataset(train_first_parties, train_second_parties, train_case_contents, train_labels)\n",
        "train_dataloader = DataLoader(train_dataset_, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "total_steps=len(train_dataloader)*num_epochs\n",
        "\n",
        "validation_first_parties=validation_dataset['first_party']\n",
        "validation_second_parties=validation_dataset['second_party']\n",
        "validation_case_contents=validation_dataset['facts']\n",
        "validation_labels=validation_dataset['first_party_winner']\n",
        "\n",
        "validation_dataset_ = CaseDataset(validation_first_parties, validation_second_parties, validation_case_contents, train_labels)\n",
        "validation_dataloader = DataLoader(validation_dataset_, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "48s84mM-zKQH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.bert = base_model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear = nn.Linear(base_model.config.hidden_size, 1)  # Adjust the input and output dimensions as needed\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.linear(pooled_output)\n",
        "        logits = self.sigmoid(logits)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "WtX7dPOUtia-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss().to(device)\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "base_model = BertModel.from_pretrained(model_name)\n",
        "model=CustomModel(base_model).to(device)\n",
        "\n",
        "# Set up the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "scheduler=get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq8wQWlsuze5",
        "outputId": "169cb710-b9e3-4e1f-be9a-611967d4c9b5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "dPo9kEPobxms"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "\n",
        "model.train()\n",
        "\n",
        "valid_losses, lowest_loss= list(), np.inf\n",
        "early_stop = 100\n",
        "progress_interval=1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss=0\n",
        "    total_correct=0\n",
        "    total_samples = 0\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels'].float()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        outputs=outputs.squeeze()\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Compute accuracy\n",
        "        predicted_labels = torch.round(outputs)\n",
        "        correct_predictions = torch.eq(predicted_labels, labels).sum().item()\n",
        "        total_correct += correct_predictions\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Compute metrics for the epoch\n",
        "    epoch_loss = total_loss / len(train_dataloader)\n",
        "    epoch_accuracy = total_correct / total_samples\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"  Loss: {epoch_loss:.4f}\")\n",
        "    print(f\"  Accuracy: {epoch_accuracy:.4f}\")\n",
        "\n",
        "    # validate the model\n",
        "    model.eval()\n",
        "    val_loss=0\n",
        "    val_samples=0\n",
        "    itr=0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_dataloader:\n",
        "          itr+=1\n",
        "          input_ids = batch['input_ids']\n",
        "          attention_mask = batch['attention_mask']\n",
        "          labels = batch['labels'].float()\n",
        "          outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "          outputs=outputs.squeeze()\n",
        "\n",
        "          loss = criterion(outputs, labels)\n",
        "          val_loss+=loss.item()\n",
        "          val_samples += labels.size(0)\n",
        "    valid_avg_loss=val_loss / itr\n",
        "    valid_losses.append(valid_avg_loss)\n",
        "\n",
        "    if valid_losses[-1] < lowest_loss:\n",
        "        lowest_loss = valid_losses[-1]\n",
        "        lowest_epoch = epoch+1\n",
        "        best_model = deepcopy(model.state_dict())\n",
        "        torch.save(model.state_dict(), file_path + 'BERT_with_lin_layer_adamw_2e5.pth')  # 모델 객체의 state_dict 저장\n",
        "    else:\n",
        "        if early_stop > 0 and lowest_epoch + early_stop < epoch:\n",
        "            print(\"Early Stopped\", epoch, \"epochs\")\n",
        "            model.load_state_dict(best_model)\n",
        "            break\n",
        "\n",
        "    if (epoch % progress_interval) == 0:\n",
        "        print(valid_losses[-1], lowest_loss, lowest_epoch, epoch+1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9wp-48ZUzNmO",
        "outputId": "b35d9e73-5935-45b5-d4b2-2cf00427fa27"
      },
      "execution_count": 46,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-a64aaca61d2f>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'input_ids': torch.tensor(inputs['input_ids'].squeeze(), device=device),\n",
            "<ipython-input-8-a64aaca61d2f>:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'attention_mask': torch.tensor(inputs['attention_mask'].squeeze(), device=device),\n",
            "<ipython-input-8-a64aaca61d2f>:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'labels': torch.tensor(label, device=device)\n",
            "<ipython-input-46-351cacc7d14e>:24: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "  Loss: 0.6430\n",
            "  Accuracy: 0.6660\n",
            "0.6334173710596177 0.6334173710596177 1 1\n",
            "Epoch 2/20\n",
            "  Loss: 0.6398\n",
            "  Accuracy: 0.6655\n",
            "0.6334173698579112 0.6334173698579112 2 2\n",
            "Epoch 3/20\n",
            "  Loss: 0.6418\n",
            "  Accuracy: 0.6665\n",
            "0.633417374424396 0.6334173698579112 2 3\n",
            "Epoch 4/20\n",
            "  Loss: 0.6396\n",
            "  Accuracy: 0.6670\n",
            "0.6334173703385938 0.6334173698579112 2 4\n",
            "Epoch 5/20\n",
            "  Loss: 0.6424\n",
            "  Accuracy: 0.6665\n",
            "0.6334173720209829 0.6334173698579112 2 5\n",
            "Epoch 6/20\n",
            "  Loss: 0.6383\n",
            "  Accuracy: 0.6665\n",
            "0.6334173741840547 0.6334173698579112 2 6\n",
            "Epoch 7/20\n",
            "  Loss: 0.6408\n",
            "  Accuracy: 0.6665\n",
            "0.633417371299959 0.6334173698579112 2 7\n",
            "Epoch 8/20\n",
            "  Loss: 0.6420\n",
            "  Accuracy: 0.6660\n",
            "0.6334173705789351 0.6334173698579112 2 8\n",
            "Epoch 9/20\n",
            "  Loss: 0.6411\n",
            "  Accuracy: 0.6665\n",
            "0.6334173688965459 0.6334173688965459 9 9\n",
            "Epoch 10/20\n",
            "  Loss: 0.6416\n",
            "  Accuracy: 0.6665\n",
            "0.6334173686562046 0.6334173686562046 10 10\n",
            "Epoch 11/20\n",
            "  Loss: 0.6428\n",
            "  Accuracy: 0.6665\n",
            "0.6334173698579112 0.6334173686562046 10 11\n",
            "Epoch 12/20\n",
            "  Loss: 0.6412\n",
            "  Accuracy: 0.6665\n",
            "0.6334173722613242 0.6334173686562046 10 12\n",
            "Epoch 13/20\n",
            "  Loss: 0.6379\n",
            "  Accuracy: 0.6670\n",
            "0.6334173725016655 0.6334173686562046 10 13\n",
            "Epoch 14/20\n",
            "  Loss: 0.6388\n",
            "  Accuracy: 0.6665\n",
            "0.6334173693772285 0.6334173686562046 10 14\n",
            "Epoch 15/20\n",
            "  Loss: 0.6416\n",
            "  Accuracy: 0.6665\n",
            "0.6334173700982525 0.6334173686562046 10 15\n",
            "Epoch 16/20\n",
            "  Loss: 0.6420\n",
            "  Accuracy: 0.6665\n",
            "0.6334173717806416 0.6334173686562046 10 16\n",
            "Epoch 17/20\n",
            "  Loss: 0.6435\n",
            "  Accuracy: 0.6660\n",
            "0.6334173722613242 0.6334173686562046 10 17\n",
            "Epoch 18/20\n",
            "  Loss: 0.6414\n",
            "  Accuracy: 0.6665\n",
            "0.6334173715403003 0.6334173686562046 10 18\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-351cacc7d14e>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     96\u001b[0m     warnings.warn(\"torch.nn.utils.clip_grad_norm is now deprecated in favor \"\n\u001b[1;32m     97\u001b[0m                   \"of torch.nn.utils.clip_grad_norm_.\", stacklevel=2)\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouped_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforeach\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_has_foreach_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_mul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_coef_clamped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-overload]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'foreach=True was passed, but can\\'t use the foreach API on {device.type} tensors'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(best_model)"
      ],
      "metadata": {
        "id": "jex-_IuBzPIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference on the test set\n",
        "test_first_parties=test_data['first_party']\n",
        "test_second_parties=test_data['second_party']\n",
        "test_case_contents=test_data['facts']\n",
        "\n",
        "test_dataset = CaseDataset(test_first_parties, test_second_parties, test_case_contents)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predicted_labels=round(outputs)\n",
        "        predictions.extend(predicted_labels.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "ZQSVshZszRpT",
        "outputId": "9b4b24d7-fdda-4bfa-8197-1ae4a326b794"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-fcf93a48a2ee>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_case_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'facts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCaseDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_first_parties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_second_parties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_case_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: CaseDataset.__init__() missing 1 required positional argument: 'labels'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "KxDa7WKa3Rfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "just_test = pd.read_csv(file_path+'test.csv')\n",
        "df = pd.DataFrame(predictions)\n",
        "df.transpose()\n",
        "just_test['first_party_winner']=df"
      ],
      "metadata": {
        "id": "PvpcCgVHzRkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "just_test.to_csv('submission_4.csv', sep=',')"
      ],
      "metadata": {
        "id": "SLHY99wKzTzD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}